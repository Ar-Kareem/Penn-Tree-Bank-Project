{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp310-cp310-macosx_10_9_x86_64.whl (9.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting numpy>=1.17.3\n",
      "  Downloading numpy-1.24.3-cp310-cp310-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /Users/edvardronglan/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Collecting scipy>=1.3.2\n",
      "  Using cached scipy-1.10.1-cp310-cp310-macosx_10_9_x86_64.whl (35.1 MB)\n",
      "Installing collected packages: threadpoolctl, numpy, scipy, scikit-learn\n",
      "Successfully installed numpy-1.24.3 scikit-learn-1.2.2 scipy-1.10.1 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "new_path = '/Users/edvardronglan/OneDrive - Massachusetts Institute of Technology/MIT/2023_spring/nlp/project/Penn-Tree-Bank-Project/nltk_data'\n",
    "nltk.data.path.append(os.path.abspath(new_path))\n",
    "from nltk.corpus import treebank\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.parse import RecursiveDescentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3914\n",
      "3914\n",
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Train a POS tagger using a combination of taggers\u001b[39;00m\n\u001b[1;32m     31\u001b[0m default_tagger \u001b[38;5;241m=\u001b[39m DefaultTagger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m unigram_tagger \u001b[38;5;241m=\u001b[39m \u001b[43mUnigramTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_tagger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m bigram_tagger \u001b[38;5;241m=\u001b[39m BigramTagger(train_set, backoff\u001b[38;5;241m=\u001b[39munigram_tagger)\n\u001b[1;32m     34\u001b[0m trigram_tagger \u001b[38;5;241m=\u001b[39m TrigramTagger(train_set, backoff\u001b[38;5;241m=\u001b[39mbigram_tagger)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/nltk/tag/sequential.py:363\u001b[0m, in \u001b[0;36mUnigramTagger.__init__\u001b[0;34m(self, train, model, backoff, cutoff, verbose)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, train\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, backoff\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cutoff\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 363\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m1\u001b[39;49m, train, model, backoff, cutoff, verbose)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/nltk/tag/sequential.py:296\u001b[0m, in \u001b[0;36mNgramTagger.__init__\u001b[0;34m(self, n, train, model, backoff, cutoff, verbose)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(model, backoff)\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m train:\n\u001b[0;32m--> 296\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(train, cutoff, verbose)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/nltk/tag/sequential.py:179\u001b[0m, in \u001b[0;36mContextTagger._train\u001b[0;34m(self, tagged_corpus, cutoff, verbose)\u001b[0m\n\u001b[1;32m    177\u001b[0m fd \u001b[39m=\u001b[39m ConditionalFreqDist()\n\u001b[1;32m    178\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m tagged_corpus:\n\u001b[0;32m--> 179\u001b[0m     tokens, tags \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39msentence)\n\u001b[1;32m    180\u001b[0m     \u001b[39mfor\u001b[39;00m index, (token, tag) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sentence):\n\u001b[1;32m    181\u001b[0m         \u001b[39m# Record the event.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m         token_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import os\n",
    "new_path = '/Users/edvardronglan/OneDrive - Massachusetts Institute of Technology/MIT/2023_spring/nlp/project/Penn-Tree-Bank-Project/nltk_data'\n",
    "nltk.data.path.append(os.path.abspath(new_path))\n",
    "from nltk.corpus import treebank\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.parse import RecursiveDescentParser\n",
    "\n",
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "\n",
    "\n",
    "sentences = treebank.sents()\n",
    "parse_trees = treebank.parsed_sents()\n",
    "tagged_sentences = treebank.tagged_sents()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "568f12863671dec1c00368c6fa998a07d47291b4ae37cbf813d8026dcd9bdb1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
