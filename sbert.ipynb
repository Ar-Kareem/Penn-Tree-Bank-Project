{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib; import psutil; _tmp = psutil.Process().parent().cmdline()\n",
    "print('CondaEnv:', pathlib.Path(_tmp[0]).parent.parent.stem, '|', [_tmp2 for _tmp2 in _tmp if '--port' in _tmp2], '|', _tmp)\n",
    "!jupyter notebook list\n",
    "!curl -sSLG localhost:8371/api/sessions --data-urlencode `jupyter notebook list | grep ':8371' | awk '/token/ {split($1,a,\"?\")} END {print a[2]}'`  | jq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<module 'dataset' from '/storage/arkareem/projects/classes/Penn-Tree-Bank-Project/dataset.py'>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try: mylibs\n",
    "except: mylibs = set()\n",
    "import importlib\n",
    "reload_my_libs = lambda : [importlib.reload(lib) for lib in mylibs]\n",
    "\n",
    "import dataset\n",
    "\n",
    "mylibs.add(dataset)\n",
    "reload_my_libs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ShardingFilterIterDataPipe,\n",
       " ShardingFilterIterDataPipe,\n",
       " ShardingFilterIterDataPipe)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext\n",
    "torchtext.datasets.PennTreebank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import PennTreebank\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Define a tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Load the Penn Treebank dataset\n",
    "train_dataset, valid_dataset, test_dataset = PennTreebank()\n",
    "\n",
    "# Build the vocabulary from the training dataset\n",
    "train_vocab = build_vocab_from_iterator(map(tokenizer, train_dataset), specials=[\"<unk>\", \"<pad>\"])\n",
    "\n",
    "# Define a function to numericalize the tokens\n",
    "def numericalize_tokens(tokens):\n",
    "    return [train_vocab.stoi[token] for token in tokens]\n",
    "\n",
    "# Define a function to preprocess a sentence\n",
    "def preprocess_sentence(sentence):\n",
    "    return numericalize_tokens(tokenizer(sentence))\n",
    "\n",
    "# Example usage:\n",
    "sentence = \"This is an example sentence.\"\n",
    "numericalized = preprocess_sentence(sentence)\n",
    "tensor = torch.tensor(numericalized)\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42068\n",
      "3370\n",
      "3761\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.datasets import PennTreebank\n",
    "\n",
    "# Load the PennTreebank dataset\n",
    "train_data = PennTreebank(split='train')\n",
    "valid_data = PennTreebank(split='valid')\n",
    "test_data = PennTreebank(split='test')\n",
    "\n",
    "# Print some examples\n",
    "print(len(list(train_data)))\n",
    "print(len(list(valid_data)))\n",
    "print(len(list(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum train 887521\n",
      "sum valid 70390\n",
      "sum test 78669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1036580"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('sum train', sum([len(sentence.split(' ')) for sentence in train_data]))\n",
    "print('sum valid', sum([len(sentence.split(' ')) for sentence in valid_data]))\n",
    "print('sum test', sum([len(sentence.split(' ')) for sentence in test_data]))\n",
    "887521+70390+78669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Tag sentences with spaCy\n",
    "for sentence in tqdm(list(train_data)):\n",
    "  doc = nlp(sentence)\n",
    "  words = [token.text for token in doc]\n",
    "  tags = [token.pos_ for token in doc]\n",
    "#   print(words, tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter\n",
      "pierre <unk> N years old will join the board as a nonexecutive director nov. N\n",
      "mr. <unk> is chairman of <unk> n.v. the dutch publishing group\n",
      "rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate\n",
      "a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported\n",
      "the asbestos fiber <unk> is unusually <unk> once it enters the <unk> with even brief exposures to it causing symptoms that show up decades later researchers said\n",
      "<unk> inc. the unit of new york-based <unk> corp. that makes kent cigarettes stopped using <unk> in its <unk> cigarette filters in N\n",
      "although preliminary findings were reported more than a year ago the latest results appear in today 's new england journal of medicine a forum likely to bring new attention to the problem\n",
      "a <unk> <unk> said this is an old story\n",
      "we 're talking about years ago before anyone heard of asbestos having any questionable properties\n",
      "there is no asbestos in our products now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/arkareem/libraries/conda/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "for i, n in enumerate(train_data):\n",
    "    print(n)\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ibm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir .data\n",
    "!wget -q -O .data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip\n",
    "!unzip -o .data/ptb.zip -d .data\n",
    "!cp .data/ptb/reader.py .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'reader' from '/storage/arkareem/projects/classes/Penn-Tree-Bank-Project/reader.py'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import reader\n",
    "import importlib\n",
    "importlib.reload(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O .data/simple-examples.tgz http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "!mkdir .data/simple-examples\n",
    "!tar xzf .data/simple-examples.tgz -C .data/simple-examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929589\n",
      "73760\n",
      "82430\n"
     ]
    }
   ],
   "source": [
    "data_dir = \".data/simple-examples/simple-examples/data/\"\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data\n",
    "print(len(train_data))\n",
    "print(len(valid_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n"
     ]
    }
   ],
   "source": [
    "def id_to_word(id_list):\n",
    "    line = []\n",
    "    for w in id_list:\n",
    "        for word, wid in word_to_id.items():\n",
    "            if wid == w:\n",
    "                line.append(word)\n",
    "    return line            \n",
    "                \n",
    "\n",
    "print(id_to_word(train_data[0:100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9970, 9971, 9972],\n",
       "        [   5, 2437,   54],\n",
       "        [   6,    1,  232],\n",
       "        [  34,   97, 4647],\n",
       "        [   0,  332, 7147],\n",
       "        [  18,  937, 1238],\n",
       "        [   2,  823,    1],\n",
       "        [   8,    1, 1716],\n",
       "        [   5,    6, 1969],\n",
       "        [  19,  484, 1112],\n",
       "        [5266,    7,  390],\n",
       "        [  53,  101,   26],\n",
       "        [ 554,   16,  850],\n",
       "        [   5,   25,  192],\n",
       "        [   1,    9,  161],\n",
       "        [  29,  466, 2028]], dtype=int32),\n",
       " array([[9971, 9972, 9974],\n",
       "        [2437,   54, 2155],\n",
       "        [   1,  232,   70],\n",
       "        [  97, 4647,   43],\n",
       "        [ 332, 7147,  328],\n",
       "        [ 937, 1238, 1340],\n",
       "        [ 823,    1,  376],\n",
       "        [   1, 1716,    6],\n",
       "        [   6, 1969,    0],\n",
       "        [ 484, 1112, 1666],\n",
       "        [   7,  390,   15],\n",
       "        [ 101,   26, 1378],\n",
       "        [  16,  850,    3],\n",
       "        [  25,  192,   18],\n",
       "        [   9,  161,  293],\n",
       "        [ 466, 2028,    7]], dtype=int32),\n",
       " (16, 3))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itera = reader.ptb_iterator(train_data, 16, 3)\n",
    "first_touple = itera.__next__()\n",
    "x = first_touple[0]\n",
    "y = first_touple[1]\n",
    "x, y, x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<module 'dataset' from '/storage/arkareem/projects/classes/Penn-Tree-Bank-Project/dataset.py'>,\n",
       " <module 'sbert' from '/storage/arkareem/projects/classes/Penn-Tree-Bank-Project/sbert.py'>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import sbert\n",
    "mylibs.add(sbert)\n",
    "reload_my_libs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33markareem\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "!wandb login  # copy the key from https://wandb.ai/authorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.get_treebank_3914()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:2'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "sbert_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/multi-qa-mpnet-base-dot-v1')\n",
    "sbert_model = AutoModel.from_pretrained('sentence-transformers/multi-qa-mpnet-base-dot-v1')\n",
    "sbert_model = sbert_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.14it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3131/3131 [00:04<00:00, 685.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 783/783 [00:01<00:00, 677.55it/s]\n"
     ]
    }
   ],
   "source": [
    "embeds, attn_masks = sbert.sbert_encode_batched(sbert_model, sbert_tokenizer, [' '.join(x) for x in data['train_sentences']], 64)\n",
    "train_embeds_pooled = sbert.pool_tokens(data['train_sentences'], embeds, attn_masks, sbert_tokenizer)\n",
    "\n",
    "embeds, attn_masks = sbert.sbert_encode_batched(sbert_model, sbert_tokenizer, [' '.join(x) for x in data['test_sentences']], 64)\n",
    "test_embeds_pooled = sbert.pool_tokens(data['test_sentences'], embeds, attn_masks, sbert_tokenizer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cuda cache\n",
    "# torch.cuda.empty_cache()\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['all_pos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing embeds would take: 1.15 GB\n",
      "Wasted: 27.4%\n",
      "torch.Size([3131, 128, 768]) torch.Size([3131, 128])\n",
      "The following sentences are longer than the maximum of 128 tokens:\n",
      "\t John William Davis , Colonsville , Miss. , fined *-4 $ 200,000 *U* ; Jeffrey Gerard Dompierre , Valrico , Fla. , $ 5,000 *U* and 10-day suspension ; Eugene Michael Felten , La Canada , Calif. , fined *-3 $ 25,000 *U* , ordered *-3 *-2 to disgorge $ 16,072 *U* and suspended *-3 one year ; Marion Stewart Spitler , La Canada , fined *-5 $ 15,000 *U* , ordered *-5 *-1 to disgorge $ 18,444 *U* and suspended *-5 six months .\n",
      "\t Charles D. Phipps Sr. , Hermitage , Pa. , fined *-1 $ 10,000 *U* ; David Scott Rankin , Lake St. Louis , Mo. , $ 15,000 *U* ; Leigh A. Sanderoff , Gaithersburg , Md. , fined *-2 $ 45,000 *U* , ordered *-2 *-3 to disgorge $ 12,252 *U* ; Sandra Ann Smith , Ridgefield , N.J. , $ 15,000 *U* ; James G. Spence , Aloha , Ore. , $ 5,000 *U* and six-month suspension ; Mona Sun , Jamaica Estates , N.Y. , $ 60,000 *U* ; William Swearingen , Minneapolis , $ 15,000 *U* and six-month suspension ; John Bew Wong , San Francisco , $ 25,000 *U* ; Rabia M. Zayed , San Francisco , $ 50,000 *U* .\n",
      "\t For years , this group included a stable of classics -- Bordeaux first growths -LRB- Lafite-Rothschild , Latour , Haut-Brion , Petrus -RRB- , Grand Cru Burgundies -LRB- Romanee-Conti and La Tache -RRB- deluxe Champagnes -LRB- Dom Perignon or Roederer Cristal -RRB- , rarefied sweet wines -LRB- Chateau Yquem or Trockenbeerenauslesen Rieslings from Germany , and Biondi-Santi Brunello Riserva from Tuscany -RRB- .\n",
      "\t Andrew Derel Adams , Killeen , Texas , fined *-1 $ 15,000 *U* ; John Francis Angier Jr. , Reddington Shores , Fla. , $ 15,000 *U* ; Mark Anthony , Arlington Heights , Ill. , $ 10,000 *U* and 30-day suspension ; William Stirlen , Arlington Heights , Ill. , $ 7,500 *U* and 30-day suspension ; Fred W. Bonnell , Boulder , Colo. , $ 2,500 *U* and six-month suspension ; Michael J. Boorse , Horsham , Pa. ; David Chiodo , Dallas , $ 5,000 *U* , barred *-3 as a principal ; Camille Chafic Cotran , London , $ 25,000 *U* ; John William Curry , fined *-4 $ 5,000 *U* , ordered *-4 *-2 to disgorge $ 30,000 *U* , one-year suspension .\n",
      "\t The following *ICH*-4 were barred *-3 or , where * noted *-1 *T*-2 , suspended *-3 and consented to findings without *-3 admitting or denying wrongdoing : Edward L. Cole , Jackson , Miss. , $ 10,000 *U* fine ; Rita Rae Cross , Denver , $ 2,500 *U* fine and 30-day suspension ; Thomas Richard Meinders , Colorado Springs , Colo. , $ 2,000 *U* fine , five-day suspension and eight-month suspension as a principal ; Ronald A. Cutrer , Baton Rouge , La. , $ 15,000 *U* fine and one-month suspension ; Karl Grant Hale , Midvale , Utah , $ 15,000 *U* fine ; Clinton P. Hayne , New Orleans , $ 7,500 *U* fine and one-week suspension ; Richard M. Kane , Coconut Creek , Fla. , $ 250,000 *U* fine ; John B. Merrick , Aurora , Colo. , $ 1,000 *U* fine and 10-day suspension ; John P. Miller , Baton Rouge , $ 2,000 *U* fine and two-week suspension ; Randolph K. Pace , New York , $ 10,000 *U* fine and 90-day suspension ; Brian D. Pitcher , New Providence , N.J. , $ 30,000 *U* fine ; Wayne A. Russo , Bridgeville , Pa. , $ 4,000 *U* fine and 15-day suspension ; Orville Leroy Sandberg , Aurora , Colo. , $ 3,500 *U* fine and 10-day suspension ; Richard T. Marchese , Las Vegas , Nev. , $ 5,000 *U* and one-year suspension ; Eric G. Monchecourt , Las Vegas , $ 5,000 *U* and one-year suspension ; and Robert Gerhard Smith , Carson City , Nev. , two-year suspension .\n",
      "\t *-1 Offering the wine at roughly $ 65 *U* a bottle wholesale -LRB- $ 100 *U* retail -RRB- , he sent merchants around the country a form asking them *-2 to check one of three answers : 1 -RRB- no , the wine is too high -LRB- 2 responses -RRB- ; 2 -RRB- yes , it 's high but I 'll take it -LRB- 2 responses -RRB- ; 3 -RRB- I 'll take all 0 I can get *T*-3 -LRB- 58 responses -RRB- .\n",
      "\t The following *ICH*-2 were neither barred nor suspended *-1 : Stephanie Veselich Enright , Rolling Hills , Calif. , fined *-3 $ 2,500 *U* and ordered *-3 *-4 to disgorge $ 11,762 *U* ; Stuart Lane Russel , Glendale , Calif. , fined *-5 $ 2,500 *U* and ordered *-5 *-6 to disgorge $ 14,821 *U* ; Devon Nilson Dahl , Fountain Valley , Calif. , fined *-7 $ 82,389 *U* .\n"
     ]
    }
   ],
   "source": [
    "embeds, attn_masks = sbert.sbert_encode_batched(sbert_model, sbert_tokenizer, [' '.join(x) for x in data['train_sentences']], 64)\n",
    "\n",
    "print(f'Storing embeds would take: {embeds.numel() * 4 / 1024 / 1024 / 1024:.2f} GB')\n",
    "print(f'Wasted: {100*attn_masks.sum().item() / attn_masks.numel():.1f}%')\n",
    "print(embeds.shape, attn_masks.shape)\n",
    "\n",
    "\n",
    "print('The following sentences are longer than the maximum of 128 tokens:')\n",
    "sent_id_overlen = torch.where(attn_masks[:, -1] == 1)[0]\n",
    "for i in sent_id_overlen:\n",
    "    print('\\t', ' '.join(data['train_sentences'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "untokenized ['<s>', 'pierre', 'vin', '##ken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'none', '##x', '##ec', '##utive', 'director', 'nov', '.', '29', '.', '</s>']\n",
      "??? what is this in the test set ['@']\n"
     ]
    }
   ],
   "source": [
    "# data['train_sentences']\n",
    "i = 0\n",
    "e = embeds[i, attn_masks[i]==1]\n",
    "e.shape[0], len(data['train_sentences'][i])\n",
    "\n",
    "print('sent', data['train_sentences'][i])\n",
    "print('untokenized', sbert_tokenizer.convert_ids_to_tokens(sbert_tokenizer.encode_plus(' '.join(data['train_sentences'][i]))['input_ids']))\n",
    "\n",
    "print('??? what is this in the test set', data['test_sentences'][739])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = sbert.ListDataset(train_embeds_pooled, data['train_tags'], data['all_pos'])\n",
    "test_dataset = sbert.ListDataset(test_embeds_pooled, data['test_tags'], data['all_pos'])\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "test_dataset = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common: 20 NN\n",
      "predicting most common pos would be 13.024323885831244\n"
     ]
    }
   ],
   "source": [
    "# get most common pos\n",
    "pos_counts = np.bincount([y for n in train_dataset.dataset.data for y in n[3]])\n",
    "print('most common:', pos_counts.argmax(), data['all_pos'][pos_counts.argmax()])\n",
    "print('predicting most common pos would be', 100*pos_counts[pos_counts.argmax()]/pos_counts.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable #p: 625,966\n"
     ]
    }
   ],
   "source": [
    "reload_my_libs()\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "model_config = {\n",
    "    'input_dim': 768, \n",
    "    # 'hidden_dims': [768],\n",
    "    'output_dim': len(data['all_pos']),\n",
    "    # 'dropout': 0.4,\n",
    "}\n",
    "\n",
    "model = sbert.SimpleModel(model_config).to(device)\n",
    "param_count = sum([p.numel() for n,p in model.named_parameters()])\n",
    "print(f'trainable #p: {param_count:,}')\n",
    "\n",
    "lr = 1e-3\n",
    "weight_decay = 0.0001\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion_ce = torch.nn.CrossEntropyLoss()\n",
    "cur_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:j4cgs38e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae1afe9a186443aa87d3be450072c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▅▆▇▇▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>98.68209</td></tr><tr><td>train_loss</td><td>0.07369</td></tr><tr><td>val_acc</td><td>91.36211</td></tr><tr><td>val_loss</td><td>0.29738</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-moon-13</strong> at: <a href='https://wandb.ai/arkareem/6.8630%20Penn%20Treebank%20Project/runs/j4cgs38e' target=\"_blank\">https://wandb.ai/arkareem/6.8630%20Penn%20Treebank%20Project/runs/j4cgs38e</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230502_052325-j4cgs38e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:j4cgs38e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/arkareem/projects/classes/Penn-Tree-Bank-Project/wandb/run-20230502_052646-atwpbomd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arkareem/6.8630%20Penn%20Treebank%20Project/runs/atwpbomd' target=\"_blank\">resilient-violet-14</a></strong> to <a href='https://wandb.ai/arkareem/6.8630%20Penn%20Treebank%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arkareem/6.8630%20Penn%20Treebank%20Project' target=\"_blank\">https://wandb.ai/arkareem/6.8630%20Penn%20Treebank%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arkareem/6.8630%20Penn%20Treebank%20Project/runs/atwpbomd' target=\"_blank\">https://wandb.ai/arkareem/6.8630%20Penn%20Treebank%20Project/runs/atwpbomd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/arkareem/6.8630%20Penn%20Treebank%20Project/runs/atwpbomd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f713c0eb6d0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"6.8630 Penn Treebank Project\", config={\n",
    "    \"architecture\": \"SimpleModel\",\n",
    "    \"dataset\": \"treebank_3914\",\n",
    "\n",
    "    \"learning_rate\": lr,\n",
    "    'weight_decay': weight_decay,\n",
    "    \"model_config\": model_config,\n",
    "    \"#params\": param_count,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(500):\n",
    "    train_acc = sbert.single_epoch(model, train_dataset, criterion_ce, epoch=cur_epoch, optim=optim, is_train=True, device=device)\n",
    "    val_acc = sbert.single_epoch(model, test_dataset, criterion_ce, epoch=cur_epoch, optim=None, is_train=False, device=device)\n",
    "    if val_acc > float(sbert.read_json_prop('best_val')):\n",
    "        sbert.write_json_prop('best_val', val_acc)\n",
    "        torch.save({'params': model.state_dict(), 'config': model_config}, 'best_model.pt')\n",
    "        print('---saved best model---')\n",
    "    cur_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Epoch: 0 Acc: 92.8% Loss: 0.255478: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 17.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92.75390529952796"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload_my_libs()\n",
    "a = torch.load('best_model.pt')\n",
    "# {'params': a, 'config': model_config}\n",
    "b = sbert.SimpleModel({\n",
    "    'input_dim': 768, \n",
    "    'hidden_dims': [512],\n",
    "    'output_dim': len(data['all_pos']),\n",
    "    'dropout': 0.2,\n",
    "}\n",
    ")\n",
    "b.load_state_dict(a)\n",
    "sbert.single_epoch(b, test_dataset, criterion_ce, epoch=0, optim=None, is_train=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent ['This', 'is', 'a', 'test', 'sentance']\n",
      "pred ['DT', 'VBZ', 'DT', 'NN', 'NNP']\n",
      "prob ['56%', '98%', '99%', '95%', '70%']\n",
      "\n",
      "sent ['This', 'is', 'a', 'valid', 'sentance']\n",
      "pred ['DT', 'VBZ', 'DT', 'JJ', 'NN']\n",
      "prob ['57%', '100%', '100%', '100%', '80%']\n",
      "\n",
      "sent ['The', 'old', 'man', 'the', 'boat']\n",
      "pred ['DT', 'NNP', 'NNP', 'DT', 'NNP']\n",
      "prob ['96%', '99%', '98%', '72%', '93%']\n",
      "\n",
      "sent ['Adam', 'and', 'Joe', 'man', 'the', 'boat']\n",
      "pred ['NNP', 'CC', 'NNP', 'VBP', 'DT', 'NN']\n",
      "prob ['100%', '90%', '100%', '57%', '97%', '86%']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_sentance = [\n",
    "    'This is a test sentance'.split(' '),\n",
    "    'This is a valid sentance'.split(' '),\n",
    "    'The old man the boat'.split(' '),\n",
    "    'Adam and Joe man the boat'.split(' '),\n",
    "]\n",
    "\n",
    "embeds, attn_masks = sbert.sbert_encode_batched(sbert_model, sbert_tokenizer, [' '.join(x) for x in new_sentance], 64, verbose=False)\n",
    "new_embeds_pooled = sbert.pool_tokens(new_sentance, embeds, attn_masks, sbert_tokenizer, verbose=False)\n",
    "with torch.no_grad():\n",
    "    new_embeds_out = [model(x) for x in new_embeds_pooled]\n",
    "\n",
    "for i in range(len(new_sentance)):\n",
    "    o = new_embeds_out[i]\n",
    "    prob_pos, pred_pos_id = torch.softmax(o, dim=1).max(dim=1)\n",
    "    pred_pos = [data['all_pos'][x] for x in pred_pos_id]\n",
    "    print('sent', new_sentance[i])\n",
    "    print('pred', pred_pos)\n",
    "    print('prob', [str(round(100*x.item()))+'%' for x in prob_pos])\n",
    "    print('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
